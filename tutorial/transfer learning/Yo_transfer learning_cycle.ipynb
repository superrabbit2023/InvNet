{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================================================================================================\n",
    "# Yufei Li\n",
    "# Princeton University\n",
    "# yl5385@princeton.edu\n",
    "\n",
    "# Feburay 2023\n",
    "\n",
    "# Note:In this demo, the neural network is synthesized using the TensorFlow (verion: 2.11.0) framework. \n",
    "# Please install TensorFlow according to the official guidance, then import TensorFlow and other dependent modules.\n",
    "# ==================================================================================================\n",
    "\n",
    "# Setup environment\n",
    "!pip install pandas numpy matplotlib\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import csv\n",
    "import math\n",
    "import cmath\n",
    "import time\n",
    "import random as python_random\n",
    "\n",
    "# reproduciable results\n",
    "def reset_seeds():\n",
    "   np.random.seed(7) \n",
    "   python_random.seed(7)\n",
    "   tf.random.set_seed(7)  \n",
    "\n",
    "inFilename = \"Input_Yo20op1084_ana_bw20_50.csv\"\n",
    "outFilename = \"Output_Yo20op1084_ana_bw20_50.csv\"\n",
    "input = pd.read_csv(inFilename,header=None)\n",
    "output = pd.read_csv(outFilename,header=None)\n",
    "inputRawTrain = []\n",
    "outputRawTrain = []\n",
    "inputRawTrain = np.array(input)\n",
    "outputRawTrain = np.array(output)\n",
    "\n",
    "inFilename = \"Input_Yo20op40_ana_50.csv\"\n",
    "outFilename = \"Output_Yo20op40_ana_50.csv\"\n",
    "input = pd.read_csv(inFilename,header=None)\n",
    "output = pd.read_csv(outFilename,header=None)\n",
    "inputTrain_1 = []\n",
    "outputTrain_1 = []\n",
    "inputTrain_1 = np.array(input)\n",
    "outputTrain_1 = np.array(output)\n",
    "\n",
    "num_inputs_train = len(inputTrain_1)\n",
    "print(\"Total Number of Training Dataset is:\",num_inputs_train)\n",
    "\n",
    "# Shuffle the dataset\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(inputTrain_1)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(outputTrain_1)\n",
    "\n",
    "# Data separation\n",
    "TRAIN_SPLIT_1 = int(0.7 * num_inputs_train)\n",
    "TRAIN_SPLIT_2 = int(0.85 * num_inputs_train)\n",
    "inputTrain, inputTest, inputs_test_2 = np.split(inputTrain_1, [TRAIN_SPLIT_1, TRAIN_SPLIT_2])\n",
    "outputTrain, outputTest, outputs_test_2 = np.split(outputTrain_1, [TRAIN_SPLIT_1, TRAIN_SPLIT_2])\n",
    "\n",
    "print(\"Dataset preparation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CYCLE = 10\n",
    "# Randomize the order of the inputs, so they can be evenly distributed for training, testing, and validation\n",
    "\n",
    "num_inputRawTrain = len(inputRawTrain)\n",
    "num_inputTrain = len(inputTrain)\n",
    "num_inputTest = len(inputTest)\n",
    "inputs_test = inputTest\n",
    "outputs_test = outputTest\n",
    "\n",
    "print(\"Total Number of Raw Training Dataset is:\",num_inputRawTrain)\n",
    "print(\"Total Number of Real Training Dataset is:\",num_inputTrain)\n",
    "print(\"Total Number of Test Dataset is:\",num_inputTest)\n",
    "\n",
    "\n",
    "inputs_train_raw = []\n",
    "outputs_train_raw = []\n",
    "inputs_train_real = []\n",
    "outputs_train_real = []\n",
    "\n",
    "\n",
    "for x in range(CYCLE):\n",
    "  tempInput_raw = copy.deepcopy(inputRawTrain)\n",
    "  tempOutput_raw = copy.deepcopy(outputRawTrain)\n",
    "  tempInput_real = copy.deepcopy(inputTrain)\n",
    "  tempOutput_real = copy.deepcopy(outputTrain)\n",
    "\n",
    "  np.random.seed(x+1)\n",
    "  np.random.shuffle(tempInput_raw)\n",
    "  np.random.seed(x+1)\n",
    "  np.random.shuffle(tempOutput_raw)\n",
    "  inputs_train_raw.append(tempInput_raw)\n",
    "  outputs_train_raw.append(tempOutput_raw)\n",
    "    \n",
    "  np.random.seed(x+1)\n",
    "  np.random.shuffle(tempInput_real)\n",
    "  np.random.seed(x+1)\n",
    "  np.random.shuffle(tempOutput_real)\n",
    "  inputs_train_real.append(tempInput_real)\n",
    "  outputs_train_real.append(tempOutput_real)\n",
    "\n",
    "print(\"Dataset randomization and separation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learning rate\n",
    "initial_learning_rate = 0.020669824682365133\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=375*65,\n",
    "    decay_rate=0.575401007701697,\n",
    "    staircase=True)\n",
    "\n",
    "# Normalization layer definition\n",
    "Normlayer1=tf.keras.layers.Normalization()\n",
    "Normlayer1.adapt(inputRawTrain)\n",
    "\n",
    "# build and then train the model\n",
    "def train_model(model,inputs_train,outputs_train,num_epoch):\n",
    "    model.add(Normlayer1)\n",
    "    model.add(tf.keras.layers.Dense(43, activation='sigmoid'))\n",
    "    model.add(tf.keras.layers.Dense(56, activation='sigmoid'))\n",
    "    model.add(tf.keras.layers.Dense(43, activation='sigmoid'))\n",
    "    model.add(tf.keras.layers.Dense(8))\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mse'])\n",
    "\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "#       save_weights_only=True,\n",
    "        verbose = 1,\n",
    "        monitor='val_mse',\n",
    "        mode='auto',\n",
    "        save_best_only=True)\n",
    "\n",
    "    history = model.fit(inputs_train, outputs_train, epochs=num_epoch, batch_size=16, validation_data=(inputs_test, outputs_test), callbacks=[model_checkpoint_callback])\n",
    "    return history\n",
    "\n",
    "# Use the model to predict test data\n",
    "def predict_with_FNN(model,inputs_test,outputs_test):\n",
    "    output_pred = model.predict(inputs_test)\n",
    "    output_diff = output_pred - outputs_test\n",
    "    AE = output_diff\n",
    "    MAE = np.mean(abs(output_diff))\n",
    "    MAE_1 =  np.mean(abs(output_pred[:,[0,2,4,6]] - outputs_test[:,[0,2,4,6]]))\n",
    "    MAE_2 =  np.mean(abs(output_pred[:,[1,3,5,7]] - outputs_test[:,[1,3,5,7]]))\n",
    "    MSE = np.square(np.subtract(output_pred,outputs_test)).mean()\n",
    "    MSE_1 = np.square(np.subtract(output_pred[:,[0,2,4,6]],outputs_test[:,[0,2,4,6]])).mean()\n",
    "    MSE_2 = np.square(np.subtract(output_pred[:,[1,3,5,7]],outputs_test[:,[1,3,5,7]])).mean()\n",
    "    \n",
    "    pct_95th_error = np.percentile(abs(output_diff),95)\n",
    "    std_error = np.std(output_diff)\n",
    "    print(\"Mean Absolute Error(MAE): %.9f\" % MAE)\n",
    "    print(\"MAE conductance: %.9f\" % MAE_1)\n",
    "    print(\"MAE susceptance: %.9f\" % MAE_2)\n",
    "    print(\"95th Error: %.9f\" % pct_95th_error)\n",
    "    print(\"Standard Deviation: %.9f\" % std_error)\n",
    "    print(\"MSE: %.9f\" % MSE)\n",
    "    print(\"MSE conductance: %.9f\" % MSE_1)\n",
    "    print(\"MSE susceptance: %.9f\" % MSE_2)\n",
    "    print(\"Absolute Error(first element, conductance):\", np.around(AE[[0],[0,2,4,6]],9))\n",
    "\n",
    "    return MAE,MAE_1,MAE_2,pct_95th_error,std_error,MSE,MSE_1,MSE_2,AE\n",
    "\n",
    "print(\"Training and prediction function definition complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "\n",
    "# initialization/getting reproducible results\n",
    "reset_seeds()\n",
    "\n",
    "# Direct Train\n",
    "mae_direct = []\n",
    "mae_direct_iteration = []\n",
    "mae_1_direct = []\n",
    "mae_1_direct_iteration = []\n",
    "mae_2_direct = []\n",
    "mae_2_direct_iteration = []\n",
    "pct95th_direct = []\n",
    "pct95th_direct_iteration = []\n",
    "std_direct = []\n",
    "std_direct_iteration = []\n",
    "mse_direct = []\n",
    "mse_direct_iteration = []\n",
    "mse_1_direct = []\n",
    "mse_1_direct_iteration = []\n",
    "mse_2_direct = []\n",
    "mse_2_direct_iteration = []\n",
    "ae_direct = []\n",
    "ae_direct_iteration = []\n",
    "\n",
    "num_data_array = [5,10,30,50,100,150,200,300,400,550]\n",
    "\n",
    "for x in range(len(num_data_array)):\n",
    "    # Set learning rate\n",
    "    initial_learning_rate = 0.020669824682365133\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=math.ceil(num_data_array[x]/16)*65,\n",
    "        decay_rate=0.575401007701697,\n",
    "        staircase=True)\n",
    "    \n",
    "    mae_direct_iteration.append(num_data_array[x])\n",
    "    mae_1_direct_iteration.append(num_data_array[x])\n",
    "    mae_2_direct_iteration.append(num_data_array[x])\n",
    "    pct95th_direct_iteration.append(num_data_array[x])\n",
    "    std_direct_iteration.append(num_data_array[x])\n",
    "    mse_direct_iteration.append(num_data_array[x])\n",
    "    mse_1_direct_iteration.append(num_data_array[x])\n",
    "    mse_2_direct_iteration.append(num_data_array[x])\n",
    "    ae_direct_iteration.append(num_data_array[x])\n",
    "    for num_train_iteration in range(10):\n",
    "        inputs_train_real_cut = inputs_train_real[num_train_iteration][0:0+num_data_array[x]]\n",
    "        outputs_train_real_cut = outputs_train_real[num_train_iteration][0:0+num_data_array[x]]\n",
    "#         inputs_train_real_cut = inputs_train_real[num_train_iteration]\n",
    "#         outputs_train_real_cut = outputs_train_real[num_train_iteration]\n",
    "        \n",
    "        model_direct = tf.keras.Sequential()\n",
    "        checkpoint_filepath = './checkpoint/direct/direct_weights.hdf5' #\n",
    "        history = train_model(model_direct,inputs_train_real_cut,outputs_train_real_cut,EPOCHS)\n",
    "        model_direct.load_weights(checkpoint_filepath) # load the best model\n",
    "        predict_results = predict_with_FNN(model_direct,inputs_test,outputs_test)\n",
    "        print(\"Number of Data Trained: %.1f\" % num_data_array[x])\n",
    "        print(\"Iteration Times: %.1f\" % num_train_iteration)\n",
    "        \n",
    "        mae_direct_iteration.append(predict_results[0])\n",
    "        mae_1_direct_iteration.append(predict_results[1])\n",
    "        mae_2_direct_iteration.append(predict_results[2])\n",
    "        pct95th_direct_iteration.append(predict_results[3])\n",
    "        std_direct_iteration.append(predict_results[4])\n",
    "        mse_direct_iteration.append(predict_results[5])\n",
    "        mse_1_direct_iteration.append(predict_results[6])\n",
    "        mse_2_direct_iteration.append(predict_results[7])\n",
    "        for i in range(num_inputTest):\n",
    "            ae_direct_iteration.extend(predict_results[8][i])\n",
    "        print()\n",
    "        \n",
    "    mae_direct.append(mae_direct_iteration)\n",
    "    mae_direct_iteration = []\n",
    "    mae_1_direct.append(mae_1_direct_iteration)\n",
    "    mae_1_direct_iteration = []\n",
    "    mae_2_direct.append(mae_2_direct_iteration)\n",
    "    mae_2_direct_iteration = []\n",
    "    pct95th_direct.append(pct95th_direct_iteration)\n",
    "    pct95th_direct_iteration = []\n",
    "    std_direct.append(std_direct_iteration)\n",
    "    std_direct_iteration = []\n",
    "    mse_direct.append(mse_direct_iteration)\n",
    "    mse_direct_iteration = []\n",
    "    mse_1_direct.append(mse_1_direct_iteration)\n",
    "    mse_1_direct_iteration = []\n",
    "    mse_2_direct.append(mse_2_direct_iteration)\n",
    "    mse_2_direct_iteration = []\n",
    "    ae_direct.append(ae_direct_iteration)\n",
    "    ae_direct_iteration = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the size of the graphs. The default size is (6,4).\n",
    "plt.rcParams[\"figure.figsize\"] = (9,6)\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'], '-r.', label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], '--b.', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.semilogy()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(plt.rcParams[\"figure.figsize\"])\n",
    "#print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS_1 = 500\n",
    "EPOCHS_2 = 500\n",
    "\n",
    "# initialization/getting reproducible results\n",
    "reset_seeds()\n",
    "\n",
    "# Pre-train\n",
    "initial_learning_rate = 0.020669824682365133\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=math.ceil(num_inputRawTrain/16)*65,\n",
    "    decay_rate=0.575401007701697,\n",
    "    staircase=True)\n",
    "model_pre = tf.keras.Sequential()\n",
    "checkpoint_filepath = './checkpoint/pre_train/pre_train_weights.hdf5' \n",
    "history_pre = train_model(model_pre,inputs_train_raw[0],outputs_train_raw[0],EPOCHS_1)\n",
    "model_pre.load_weights(checkpoint_filepath) # load the best model\n",
    "predict_with_FNN(model_pre,inputs_test,outputs_test)\n",
    "print()\n",
    "\n",
    "# Train with experimental data\n",
    "mae_pretrain = []\n",
    "mae_pretrain_iteration = []\n",
    "mae_1_pretrain = []\n",
    "mae_1_pretrain_iteration = []\n",
    "mae_2_pretrain = []\n",
    "mae_2_pretrain_iteration = []\n",
    "pct95th_pretrain = []\n",
    "pct95th_pretrain_iteration = []\n",
    "std_pretrain = []\n",
    "std_pretrain_iteration = []\n",
    "mse_pretrain = []\n",
    "mse_pretrain_iteration = []\n",
    "mse_1_pretrain = []\n",
    "mse_1_pretrain_iteration = []\n",
    "mse_2_pretrain = []\n",
    "mse_2_pretrain_iteration = []\n",
    "ae_pretrain = []\n",
    "ae_pretrain_iteration = []\n",
    "\n",
    "num_data_array = [5,10,30,50,100,150,200,300,400,550]\n",
    "\n",
    "for x in range(len(num_data_array)):\n",
    "    # Set learning rate\n",
    "    initial_learning_rate = 0.020669824682365133\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=math.ceil(num_data_array[x]/16)*65,\n",
    "        decay_rate=0.575401007701697,\n",
    "        staircase=True)\n",
    "    \n",
    "    mae_pretrain_iteration.append(num_data_array[x])\n",
    "    mae_1_pretrain_iteration.append(num_data_array[x])\n",
    "    mae_2_pretrain_iteration.append(num_data_array[x])\n",
    "    pct95th_pretrain_iteration.append(num_data_array[x])\n",
    "    std_pretrain_iteration.append(num_data_array[x])\n",
    "    mse_pretrain_iteration.append(num_data_array[x])\n",
    "    mse_1_pretrain_iteration.append(num_data_array[x])\n",
    "    mse_2_pretrain_iteration.append(num_data_array[x])\n",
    "    ae_pretrain_iteration.append(num_data_array[x])\n",
    "    for num_train_iteration in range(10):\n",
    "        # load pre-trained model\n",
    "        pre_train_model_path = './checkpoint/pre_train/pre_train_weights.hdf5'  \n",
    "        model_pre.load_weights(pre_train_model_path) # load the best model\n",
    "        \n",
    "        # prepare experimental data\n",
    "        inputs_train_real_cut = inputs_train_real[num_train_iteration][0:0+num_data_array[x]]\n",
    "        outputs_train_real_cut = outputs_train_real[num_train_iteration][0:0+num_data_array[x]]\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "        model_pre.compile(optimizer=opt, loss='mse', metrics=['mse']) \n",
    "        checkpoint_filepath = './checkpoint/pre_train/final_weights.hdf5' \n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_filepath,\n",
    "            verbose = 1,\n",
    "            monitor='val_mse',\n",
    "            mode='auto',\n",
    "            save_best_only=True)\n",
    "        \n",
    "        history = model_pre.fit(inputs_train_real_cut, outputs_train_real_cut, epochs=EPOCHS_2, batch_size=16, validation_data=(inputs_test, outputs_test), callbacks=[model_checkpoint_callback])\n",
    "        model_pre.load_weights(checkpoint_filepath) # load the best model\n",
    "        predict_results = predict_with_FNN(model_pre,inputs_test,outputs_test)\n",
    "        print(\"Number of Data Trained: %.1f\" % num_data_array[x])\n",
    "        print(\"Iteration Times: %.1f\" % num_train_iteration)\n",
    "        \n",
    "        mae_pretrain_iteration.append(predict_results[0])\n",
    "        mae_1_pretrain_iteration.append(predict_results[1])\n",
    "        mae_2_pretrain_iteration.append(predict_results[2])\n",
    "        pct95th_pretrain_iteration.append(predict_results[3])\n",
    "        std_pretrain_iteration.append(predict_results[4])\n",
    "        mse_pretrain_iteration.append(predict_results[5])\n",
    "        mse_1_pretrain_iteration.append(predict_results[6])\n",
    "        mse_2_pretrain_iteration.append(predict_results[7])\n",
    "        for i in range(num_inputTest):\n",
    "            ae_pretrain_iteration.extend(predict_results[8][i])\n",
    "        print()\n",
    "    mae_pretrain.append(mae_pretrain_iteration)\n",
    "    mae_pretrain_iteration = []\n",
    "    mae_1_pretrain.append(mae_1_pretrain_iteration)\n",
    "    mae_1_pretrain_iteration = []\n",
    "    mae_2_pretrain.append(mae_2_pretrain_iteration)\n",
    "    mae_2_pretrain_iteration = []\n",
    "    pct95th_pretrain.append(pct95th_pretrain_iteration)\n",
    "    pct95th_pretrain_iteration = []\n",
    "    std_pretrain.append(std_pretrain_iteration)\n",
    "    std_pretrain_iteration = []\n",
    "    mse_pretrain.append(mse_pretrain_iteration)\n",
    "    mse_pretrain_iteration = []\n",
    "    mse_1_pretrain.append(mse_1_pretrain_iteration)\n",
    "    mse_1_pretrain_iteration = []\n",
    "    mse_2_pretrain.append(mse_2_pretrain_iteration)\n",
    "    mse_2_pretrain_iteration = []\n",
    "    ae_pretrain.append(ae_pretrain_iteration)\n",
    "    ae_pretrain_iteration = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the size of the graphs. The default size is (6,4).\n",
    "plt.rcParams[\"figure.figsize\"] = (9,6)\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history_pre.history['loss'], '-r.', label='Training Loss')\n",
    "plt.plot(history_pre.history['val_loss'], '--b.', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.semilogy()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(plt.rcParams[\"figure.figsize\"])\n",
    "#print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the size of the graphs. The default size is (6,4).\n",
    "plt.rcParams[\"figure.figsize\"] = (9,6)\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'], '-r.', label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], '--b.', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.semilogy()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(plt.rcParams[\"figure.figsize\"])\n",
    "#print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creat new directory\n",
    "# import os\n",
    "# os.mkdir('results')\n",
    "import os\n",
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')\n",
    "# Save Mean Squared Error of pretrained results\n",
    "with open('./results/mse_results_pretrain.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(mse_pretrain)\n",
    "\n",
    "# Save Mean Squared Error of non-pretrained results\n",
    "with open('./results/mse_results_nopre.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(mse_direct)\n",
    "    \n",
    "# Save Mean Squared Error of pretrained results\n",
    "with open('./results/mse_1_results_pretrain.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(mse_1_pretrain)\n",
    "\n",
    "# Save Mean Squared Error of non-pretrained results\n",
    "with open('./results/mse_1_results_nopre.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(mse_1_direct)\n",
    "\n",
    "# Save Mean Squared Error of pretrained results\n",
    "with open('./results/mse_2_results_pretrain.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(mse_2_pretrain)\n",
    "\n",
    "# Save Mean Squared Error of non-pretrained results\n",
    "with open('./results/mse_2_results_nopre.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(mse_2_direct)    \n",
    "    \n",
    "# Save Mean Absolute Error of pretrained results\n",
    "with open('./results/mae_results_pretrain.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(mae_pretrain)\n",
    "\n",
    "# Save Mean Absolute Error of non-pretrained results\n",
    "with open('./results/mae_results_nopre.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(mae_direct)\n",
    "    \n",
    "# Save Mean Absolute Error of pretrained results\n",
    "with open('./results/mae_1_results_pretrain.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(mae_1_pretrain)\n",
    "\n",
    "# Save Mean Absolute Error of non-pretrained results\n",
    "with open('./results/mae_1_results_nopre.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(mae_1_direct)\n",
    "    \n",
    "# Save Mean Absolute Error of pretrained results\n",
    "with open('./results/mae_2_results_pretrain.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(mae_2_pretrain)\n",
    "\n",
    "# Save Mean Absolute Error of non-pretrained results\n",
    "with open('./results/mae_2_results_nopre.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(mae_2_direct)\n",
    "    \n",
    "# Save Absolute Error of pretrained results\n",
    "with open('./results/ae_results_pretrain.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(ae_pretrain)\n",
    "\n",
    "# Save Absolute Error of non-pretrained results\n",
    "with open('./results/ae_results_nopre.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(ae_direct)\n",
    "    \n",
    "# Save 95th Percentage Error of pretrained results\n",
    "with open('./results/pct95th_results_pretrain.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(pct95th_pretrain)\n",
    "\n",
    "# Save 95th Percentage Error of non-pretrained results\n",
    "with open('./results/pct95th_results_nopre.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(pct95th_direct)\n",
    "    \n",
    "# Save Standard Deviation of pretrained results\n",
    "with open('./results/std_results_pretrain.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(std_pretrain)\n",
    "\n",
    "# Save Standard Deviation of non-pretrained results\n",
    "with open('./results/std_results_nopre.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(std_direct)\n",
    "\n",
    "print(\"File generation complete!\")     "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": [
    {
     "file_id": "1kNYLYMj2vowrtHEcjI6ebAbwCWdHIR4_",
     "timestamp": 1601231066833
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
